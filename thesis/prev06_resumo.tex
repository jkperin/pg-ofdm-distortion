%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% resumo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{resumo}
 \begin{Spacing}{1.5}
Neste trabalho será estudada uma teoria de grandes sinais para modelar a propagação dispersiva de sinais OFDM (\textit{Orthogonal Frequency Division Multiplexing}) em sistemas ópticos IMDD (\textit{Intensity Modulation - Direct Detection}) empregando lasers modulados diretamente com o objetivo de calcular as distorções causadas pelo efeito combinado entre \emph{chirp} do laser semicondutor e dispersão cromática. A base da teoria apresentada neste trabalho é derivada de um artigo recente, onde são apresentadas equações analíticas para o cálculo da corrente detectada em sistemas IMDD com lasers modulados diretamente por 1 e por \emph{N} componentes sinusoidais. A partir da corrente detectada pode-se calcular as distorções induzidas pelo \emph{chirp} do laser. No entanto, a teoria desenvolvida originalmente não foi desenvolvida especificamente para sinais OFDM, o que torna impraticável o cálculo das distorções induzidas pelo \emph{chirp} em sinais OFDM com centenas de subportadoras. Para contornar esse problema, neste trabalho serão propostas simplificações a essas equações de forma a possibilitar o cálculo, porém sem prejudicar a precisão do modelo.

Neste trabalho também será proposto uma teoria de grandes sinais mais precisa do que a apresentada originalmente. A maior precisão é conseguida em troca da maior complexidade das equações. Como consequência, a teoria desenvolvida não pode ser aplicada em sinais OFDM com algumas dezenas de subportadoras.

Por fim, a teoria proposta é aplicada a sinais OFDM de interesse de pesquisa e comercial, mostrando que as simplificações não comprometem a precisão da teoria de grandes sinais utilizada. Para todos os casos analisados, o erro calculado em relação aos valores esperados pela teoria foram inferiores a 3 dB.
\end{Spacing}
\end{resumo}
